---
title: "ProbSet 7, February 27"
author: 
  - Gideon Gordon
  - <gideongordon2029@u.northwestern.edu>
date: February 27, 2026
date-format: "**[D]ue:** MMMM D, YYYY"
format: 
     pdf:
       documentclass: article
fontsize: 12pt
urlcolor: blue
number-sections: true
geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
    - \usepackage{float}
    - \floatplacement{figure}{t}
    - \floatplacement{table}{t}
    - \usepackage{flafter}
    - \usepackage[T1]{fontenc}
    - \usepackage[utf8]{inputenc}
    - \usepackage{ragged2e}
    - \usepackage{booktabs}
    - \usepackage{amsmath}
    - \usepackage{url}
---

**Submission**: [https://canvas.northwestern.edu/courses/245562/assignments/1687752](https://canvas.northwestern.edu/courses/245562/assignments/1687752)

---

# Problem 1


1. Define statistical power in your own words.
Statistical power is the likelihood that, given a certain experimental setup, a certain acceptable alpha-level, and a certain actual effect size, the researcher will *correctly* reject the null hypothesis. That is, the experimental setup can correctly determine that the effect is statistically different from zero. 

2. Explain the relationship between Type I error ($\alpha$), Type II error ($\beta$), and power.
A Type I error is where we incorrectly believe that a relationship is real when it is not (we incorrectly reject the null hypothesis). A Type II error is where we incorrectly believe that a relationship is not real when it totally is (we incorrectly fail to reject the null hypothesis). 

---

# Problem 2

## 2a.

```{r}
  significant_outcomes = c()
  p_values = c()
  n_sim = 100
  for (i in n_sim) {
  x = c(rnorm(mean = 0, n = 100, sd = 1))
  y = c(rnorm(mean = true_effect*x, n = 100, sd = 1))
  simulated_data = data.frame(x = x, y = y)
  simulated_model = lm(y ~ x, data = simulated_data)
  p_value_i = summary(simulated_model)$coefficients[, 4]["x"]
  p_values = append(p_values, p_value_i)
  }
  significant_outcomes = p_values < 0.05
  power = sum(significant_outcomes) / ncol(significant_outcomes)
  print(power)

```


Simulate power for different scenarios:

```{r problem2a1, eval=FALSE}
# Complete this code to simulate power for different sample sizes
  # 1. Simulate n_sim datasets with given parameters
  # 2. For each dataset, run linear regression
  # 3. Calculate proportion of simulations where p < alpha
  # 4. Return power estimate
set.seed(112)

simulate_power <- function(true_effect, n, sigma = 1, alpha = 0.05, n_sim = 1000) {
  significant_outcomes = vector()
  p_values = vector()
  for (i in n_sim) {
  x = c(rnorm(mean = 0, n = n, sd = sigma))
  y = c(rnorm(mean = true_effect*x, n = n, sd = sigma))
  simulated_data = data.frame(x = x, y = y)
  simulated_model = lm(y ~ x, data = simulated_data)
  p_value_i = summary(simulated_model)$coefficients[, 4]["x"]
  p_values = c(p_values, p_value_i)
  }
  power = mean(p_values < alpha)
  print(power)
}

simulate_power(0.2, 100, sigma = 1)


```

```{r}
```


```{r problem2a2, eval=FALSE}

# Test for different sample sizes
sample_sizes <- c(50, 100, 200, 400, 800)
true_effect <- 0.2

# Create a data frame with power estimates for each sample size
power_results <- data.frame()
for (n in sample_sizes) {
  # Your code here
}

# Create visualization
library(ggplot2)
# Plot power vs sample size
```

**Questions:**
1. What sample size is needed to achieve 80% power for detecting an effect of 0.2?

2. How does changing the true effect size to 0.4 affect the required sample size?

3. What happens to power if you double the variance (sigma)?


## 2b.

**Questions:**
1. What is the "winner's curse" and why does it occur?
A problem that comes with insufficient power is not just that we will make Type II errors and fail to distinguish a real effect from zero. We also find that when our confidence intervals are very wide because of a small sample size or large variance, 

2. How does sample size affect the magnitude of the winner's curse?


---

# Problem 3

## 3a.

1. Define and distinguish between:
   - Moderator variable
   - Mediator variable
2. Draw path diagrams for both (like in the slides)
3. Provide a political science example of each

## 3b.

Using the QOG data from the slides:

```{r problem3b, eval=FALSE}
library(rqog)
library(dplyr)
library(ggplot2)

# Load and prepare data
qog_data <- read_qog(which_data = "standard", data_type = "time-series")

# Create analysis dataset
analysis_data <- qog_data %>%
  select(
    country = cname,
    year = year,
    democracy = vdem_libdem,
    gdp_pc = gle_cgdpc,
    colonial = ht_colonial
  ) %>%
  filter(!is.na(democracy), !is.na(gdp_pc), !is.na(colonial)) %>%
  group_by(country) %>%
  filter(year == max(year)) %>%
  ungroup() %>%
  mutate(
    log_gdp = log(gdp_pc),
    colonized = ifelse(colonial > 0, 1, 0)
  )

# Your tasks:
# 1. Run two models:
#    a. Main effects only: democracy ~ log_gdp + colonized
#    b. With interaction: democracy ~ log_gdp * colonized

# 2. Calculate and interpret:
#    a. The marginal effect of log_gdp when colonized = 0
#    b. The marginal effect of log_gdp when colonized = 1
#    c. Test whether these effects are statistically different

# 3. Create visualization:
#    a. Plot with two regression lines (one for each colonized status)
#    b. Include confidence bands
#    c. Add appropriate labels and title
```

**Questions:**
1. How does the relationship between GDP and democracy differ between former colonies and never-colonized countries?

2. Is the interaction statistically significant? What does this mean substantively?


