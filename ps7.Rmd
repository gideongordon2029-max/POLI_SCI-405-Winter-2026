---
title: "ProbSet 7, February 27"
author: 
  - Gideon Gordon
  - <gideongordon2029@u.northwestern.edu>
date: February 27, 2026
date-format: "**[D]ue:** MMMM D, YYYY"
format: 
     pdf:
       documentclass: article
fontsize: 12pt
urlcolor: blue
number-sections: true
geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
    - \usepackage{float}
    - \floatplacement{figure}{t}
    - \floatplacement{table}{t}
    - \usepackage{flafter}
    - \usepackage[T1]{fontenc}
    - \usepackage[utf8]{inputenc}
    - \usepackage{ragged2e}
    - \usepackage{booktabs}
    - \usepackage{amsmath}
    - \usepackage{url}

---

**Submission**: [https://canvas.northwestern.edu/courses/245562/assignments/1687752](https://canvas.northwestern.edu/courses/245562/assignments/1687752)

```{r setup}
library(dplyr)
library(ggplot2)
library(DiagrammeRsvg)
library(knitr)
library(webshot2)
```


---

# Problem 1


1. Define statistical power in your own words.
Statistical power is the likelihood that, given a certain experimental setup, a certain acceptable alpha-level, and a certain actual effect size, the researcher will *correctly* reject the null hypothesis. That is, the experimental setup can correctly determine that the effect is statistically different from zero. 

2. Explain the relationship between Type I error ($\alpha$), Type II error ($\beta$), and power.
A Type I error is where we incorrectly believe that a relationship is real when it is not (we incorrectly reject the null hypothesis). A Type II error is where we incorrectly believe that a relationship is not real when it totally is (we incorrectly fail to reject the null hypothesis). 

---

# Problem 2

## 2a.

Simulate power for different scenarios:

```{r problem2a1}
# Complete this code to simulate power for different sample sizes
  # 1. Simulate n_sim datasets with given parameters
  # 2. For each dataset, run linear regression
  # 3. Calculate proportion of simulations where p < alpha
  # 4. Return power estimate
set.seed(144)

simulate_power <- function(true_effect, sample_size, sigma = 1, alpha = 0.05, n_sim = 1000) {
  significant_count = 0
  
  for (i in 1:n_sim) {
  x = c(rnorm(n = sample_size))
  y = c(true_effect*x + rnorm(n = sample_size, sd = sigma))
  simulated_data = data.frame(x = x, y = y)
  simulated_model = lm(y ~ x, data = simulated_data)
  p_value_i = summary(simulated_model)$coefficients[2, 4]
  if (p_value_i < alpha) {significant_count = significant_count + 1}
  }
  significant_count / n_sim
}

simulate_power(0.2, 100, sigma = 1, alpha = 0.05, n_sim = 1000)

```


```{r problem2a2}

# Test for different sample sizes
sample_sizes <- c(50, 100, 200, 400, 800)
true_effects <- c(0.2, 0.4)
sigmas = c(1, 2)
n_sim = 1000
```

```{r problem2a3}
# Create a data frame with power estimates for each sample size
simulated_power_sample_size = data.frame(sample_sizes) 

count = 0
powers_0.2 = c()
for (i in sample_sizes){
  count = count + 1
  power = simulate_power(true_effect = 0.2, sample_size = sample_sizes[count], sigma = 1, alpha = 0.05, n_sim = 1000)
  powers_0.2 = c(powers_0.2, power)
}
simulated_power_sample_size_0.2 = data.frame(sample_sizes, powers_0.2)

count = 0
powers_0.4 = c()
for (i in sample_sizes){
  count = count + 1
  power = simulate_power(true_effect = 0.4, sample_size = sample_sizes[count], sigma = 1, alpha = 0.05, n_sim = 1000)
  powers_0.4 = c(powers_0.4, power)
}
simulated_power_sample_size_0.4 = data.frame(sample_sizes, powers_0.4) 


```

```{r problem2a4_visualizing}

# Create visualization
library(ggplot2)
# Plot power vs sample size
graph_simulated_power_sample_size = ggplot() +
  geom_line(data = simulated_power_sample_size_0.2, aes(x = sample_sizes, y = powers_0.2), color = "blue") +
  geom_line(data = simulated_power_sample_size_0.4, aes(x = sample_sizes, y = powers_0.4), color = "red") + 
  labs(x = "Simulated Power", y = "Sample Size", title = "Simulated power by sample size with effect sizes 0.2  (blue) and 0.4 (red)")

graph_simulated_power_sample_size
```

(I do not go for creativity in labeling this graph).

**Questions:**
1. What sample size is needed to achieve 80% power for detecting an effect of 0.2?

You need at least 400 observations. 

2. How does changing the true effect size to 0.4 affect the required sample size?

The required sample size goes way down, from 400 to 100; doubling the effect reduces the required sample size by a factor of four.  

3. What happens to power if you double the variance (sigma)?

Let's find out! 

```{r}
count = 0
powers_0.2_sigma2 = c()
for (i in sample_sizes){
  count = count + 1
  power = simulate_power(true_effect = 0.2, sample_size = sample_sizes[count], sigma = 2, alpha = 0.05, n_sim = 1000)
  powers_0.2_sigma2 = c(powers_0.2_sigma2, power)
}
simulated_power_sample_size_0.2_sigma2 = data.frame(sample_sizes, powers_0.2_sigma2)

count = 0
powers_0.4_sigma2 = c()
for (i in sample_sizes){
  count = count + 1
  power = simulate_power(true_effect = 0.4, sample_size = sample_sizes[count], sigma = 2, alpha = 0.05, n_sim = 1000)
  powers_0.4_sigma2 = c(powers_0.4_sigma2, power)
}
simulated_power_sample_size_0.4_sigma2 = data.frame(sample_sizes, powers_0.4_sigma2) 
```

```{r}
# Plot power vs sample size
colors = c("powers_0.2" = "blue", "powers_0.2_sigma2" = "lightblue", "powers_0.4" = "red", "powers_0.4_sigma2" = "pink")

graph_simulated_power_sample_size_and_variance = ggplot() +
  scale_color_manual(values = colors, labels = c("Effect of 0.2 and sigma of 1", "Effect of 0.2 and sigma of 2", "Effect of 0.4 and sigma of 1", "Effect of 0.4 and sigma of 2")) +
  geom_line(data = simulated_power_sample_size_0.2, aes(x = sample_sizes, y = powers_0.2, color = "powers_0.2")) +
  geom_line(data = simulated_power_sample_size_0.2_sigma2, aes(x = sample_sizes, y = powers_0.2_sigma2, color = "powers_0.2_sigma2")) + 
  geom_line(data = simulated_power_sample_size_0.4, aes(x = sample_sizes, y = powers_0.4, color = "powers_0.4"))+ 
    geom_line(data = simulated_power_sample_size_0.4_sigma2, aes(x = sample_sizes, y = powers_0.4_sigma2, color = "powers_0.4_sigma2")) + 
  labs(
    x = "Simulated Power", 
    y = "Sample Size", 
    title = "Simulated power by sample size with effect sizes 0.2 and 0.4"
    )

graph_simulated_power_sample_size_and_variance

```

So what we can see from this graph is that doubling the sigma basically cancels out the effect of doubling the effect. A sample size four times larger is required when variance is doubled. 

## 2b.

**Questions:**
1. What is the "winner's curse" and why does it occur?

A problem that comes with insufficient power is not just that we will make Type II errors and fail to distinguish a real effect from zero. We also find that when our confidence intervals are very wide (because of a small sample size or large variance) and our predicted effect fairly small, any "successes" we get even in the right direction will always be massive overestimates. With large confidence intervals, small true effects are indistinguishable from zero.   

2. How does sample size affect the magnitude of the winner's curse?

If you can increase your sample size, you reduce the magnitude of the problem, since you become better able to identify smaller real effects as your confidence intervals shrink. 

---

# Problem 3

## 3a.

1. Define and distinguish between:
   - Moderator variable
   - Mediator variable
2. Draw path diagrams for both (like in the slides).

First, let's include a moderator...
```{r, echo = FALSE, eval=TRUE, fig.retina = 1, fig.align='center'}
library(DiagrammeR)
library(DiagrammeRsvg)

# Moderator diagram
moderator_diagram <- grViz("
digraph {
  graph [rankdir = LR, layout = dot]
  node [shape = box, style = filled, fillcolor = lightblue]
  
  X [label = 'X (My grades on the quiz)']
  Y [label = 'Y (My mood at end of day)']
  M [label = 'M (Whether I ate breakfast today)', fillcolor = lightyellow]
  
  // Main effect
  X -> Y [label = 'β₁']
  
  // Moderation effect
  M -> X [label = 'Moderates', style = dashed, color = red]
  
  // Direct effect of moderator
  M -> Y [label = 'β₂']
}
")

# Display in RMarkdown
moderator_diagram
```
```{r, echo=FALSE, eval=TRUE, fig.retina = 1, fig.align='center'}
# Mediator diagram
mediator_diagram <- grViz("
digraph {
  graph [rankdir = LR, layout = dot]
  node [shape = box, style = filled, fillcolor = lightblue]
  
  X [label = 'X (Whether I had breakfast)']
  Y [label = 'Y (My mood at end of day)']
  M [label = 'M (My energy during the day)', fillcolor = lightyellow]
  
  // Effect of X on mediator
  X -> M [label = 'α']
  
  // Effect of mediator on Y
  M -> Y [label = 'β']
}
")

# Display in RMarkdown
mediator_diagram
```

3. Provide a political science example of each.

A mediator: According to White and Laird, close ties to other Black people are what translate broader injunctive norms into "racialized social constraint" in the political behaviors of Black people. So, social norms influence individual behavior *through* homophilous social networks.  

A moderator: According to Sarah Parkinson's study of Palestinian insurgency in Lebanon, the structure of preexisting social networks *influences* the effect of state repression on political structure.

## 3b.

Using the QOG data from the slides:

```{r problem3b1}
library(rqog)
library(dplyr)
library(ggplot2)

# Load and prepare data
qog_data <- read_qog(which_data = "standard", data_type = "time-series")

# Create analysis dataset
analysis_data <- qog_data %>%
  select(
    country = cname,
    year = year,
    democracy = vdem_libdem,
    gdp_pc = gle_cgdpc,
    colonial = ht_colonial
  ) %>%
  filter(!is.na(democracy), !is.na(gdp_pc), !is.na(colonial)) %>%
  group_by(country) %>%
  filter(year == max(year)) %>%
  ungroup() %>%
  mutate(
    log_gdp = log(gdp_pc),
    colonized = ifelse(colonial > 0, 1, 0)
  )
```

```{r problem3b2}
library(sjPlot)
library(sjmisc)
# Your tasks:
# 1. Run two models:
#    a. Main effects only: democracy ~ log_gdp + colonized
#    b. With interaction: democracy ~ log_gdp * colonized

main_effects = lm(democracy ~ log_gdp + colonized, data = analysis_data)
interaction_effects = lm(democracy ~ log_gdp + colonized + log_gdp*colonized, data = analysis_data)

summary(main_effects)
summary(interaction_effects)
```

```{r}
# 2. Calculate and interpret:
#    a. The marginal effect of log_gdp when colonized = 0

#    b. The marginal effect of log_gdp when colonized = 1

#    c. Test whether these effects are statistically different
var(analysis_data$democracy)*1.92
```

When `colonized` = 0, the marginal effect of `log_gdp` (the effect of a one-unit increase) is 0.175. When `colonized` = 1, the effect is 0.175 for a one unit increase in `log_gdp`, and - 0.131 for a one-unit increase in `log_gdp*colonized`. 

I'm not sure how to test if these are statistically different. The variance in democracy in the data is 0.0728. I'm going to steal from Professor Seawright again. 

```{r, echo=FALSE}
meplot <- function(model,var1,var2,int,vcov,ci=.95,
                   xlab=var2,ylab=paste("Marginal Effect of",var1),
                   main="Marginal Effect Plot",
                   me_lty=1,me_lwd=1,me_col="black",
                   ci_lty=1,ci_lwd=.5,ci_col="black",
                   yint_lty=2,yint_lwd=1,yint_col="black"){
  require(ggplot2)
  alpha <- 1-ci
  z <- qnorm(1-alpha/2)
  beta.hat <- coef(model)
  cov <- vcov
  z0 <- seq(min(model.frame(model)[,var2],na.rm=T),max(model.frame(model)[,var2],na.rm=T),length.out=1000)
  dy.dx <- beta.hat[var1] + beta.hat[int]*z0
  se.dy.dx <- sqrt(cov[var1,var1] + z0^2*cov[nrow(cov),ncol(cov)] + 2*z0*cov[var1,ncol(cov)])
  upr <- dy.dx + z*se.dy.dx
  lwr <- dy.dx - z*se.dy.dx
  ggplot(data=NULL,aes(x=z0, y=dy.dx)) +
    labs(x=xlab,y=ylab,title=main) +
    geom_line(aes(z0, dy.dx),size = me_lwd, 
              linetype = me_lty, 
              color = me_col) +
    geom_line(aes(z0, lwr), size = ci_lwd, 
              linetype = ci_lty, 
              color = ci_col) +
    geom_line(aes(z0, upr), size = ci_lwd, 
              linetype = ci_lty, 
              color = ci_col) +
    geom_hline(yintercept=0,linetype=yint_lty,
               size=yint_lwd,
               color=yint_col)}
```

```{r, echo=FALSE}
meplot(interaction_effects, var1 = "log_gdp", var2 = "colonized", int = "log_gdp:colonized", vcov = vcov(interaction_effects))
```
Having shamelessly pilfered this convenient program, I can tell that the marginal effect of `log_gdp` when `colonized` is 0 is about 0.17, with a lower bound at about 0.125, while the marginal effect of `log_gdp` when `colonized` is 1 is about 0.05, with an upper bound at about 0.07, so the two values do not overlap and appear to be statistically different. 

```{r, echo=FALSE}
# 3. Create visualization:
#    a. Plot with two regression lines (one for each colonized status)
your_theory_is_eurocentric = plot_model(model = interaction_effects, type = "pred", terms = c("log_gdp", "colonized"), title = "Modernization theory: Just for the non-colonized world?", axis.title = c("Log of GDP per capita", "VDem scores"))
your_theory_is_eurocentric
#    b. Include confidence bands

#    c. Add appropriate labels and title

```

**Questions:**
1. How does the relationship between GDP and democracy differ between former colonies and never-colonized countries?
Basically, the effect of GDP per capita on democracy is *only* significant in the non-colonized countries. Outside the colonized world, rich countries are no more democratic than poor countries. 

2. Is the interaction statistically significant? What does this mean substantively?
The interaction is statistically significant, which means that, substantively, having a colonial history may in fact moderate the effects of economic prosperity on democracy. 


